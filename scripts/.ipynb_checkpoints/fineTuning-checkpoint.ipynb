{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf947308-0a96-475e-940b-c643fc1d0de1",
   "metadata": {},
   "source": [
    "# Linear layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed6d15-48fd-4c0e-9ee6-e7ad1af64277",
   "metadata": {},
   "source": [
    "Pour entraîner efficacement notre modèle à l'aide de la méthode LoRA, il est essentiel d'identifier les couches linéaires du modèle qui présentent la plus grande efficacité d'adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d9a918-fc43-4ab8-8f8d-cc1978a5f894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beyrem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForSpeechSeq2Seq, WhisperTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9884506-9103-42fe-836d-f536f2f7a26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-medium.en\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype, \n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True, \n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model_modules = str(model.modules)\n",
    "pattern = r'\\((\\w+)\\): Linear'\n",
    "linear_layer_names = re.findall(pattern, model_modules)\n",
    "\n",
    "names = []\n",
    "for name in linear_layer_names:\n",
    "    names.append(name)\n",
    "target_modules = list(set(names))\n",
    "target_modules = [\"q_proj\", \"v_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba003f2-acc6-411c-9142-c5e52b691c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd9b08a-898e-460a-a887-63b6b2174007",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                         # Rang LoRA Plus grand = plus de paramètres entraînés, meilleure performance, Temps d'éxecution plus élévés\n",
    "    lora_alpha=8,                \n",
    "    target_modules=target_modules, # Les couches que nous avons identifiées précédemment\n",
    "    lora_dropout=0.2,            \n",
    "    bias=\"none\",                  \n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db9b728-8c30-4bfb-8304-39d71bd3a037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 768,575,488 || trainable%: 0.6139\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model.enable_input_require_grads()\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Afficher la différence : Seuls les paramètres LoRA sont entraînés\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4df3d7-8301-4078-a4b6-37ecba12d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 195\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 22\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "datasets = load_dataset(\"json\", data_files={\"train\": \"../dataset/processed/json_format/train.json\", \"validation\": \"../dataset/processed/json_format/val.json\"})\n",
    "\n",
    "\n",
    "datasets = datasets.cast_column(\n",
    "    \"audio\", \n",
    "    Audio(sampling_rate=16000)\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52cb3d89-4749-483a-8886-e66489652165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch, processor):\n",
    "    audio = batch[\"audio\"]\n",
    "    \n",
    "    # feature_extractor() convertit l'array audio en spectrograms\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "\n",
    "    batch[\"labels\"] = processor.tokenizer(\n",
    "        batch[\"text\"]\n",
    "    ).input_ids\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07d4d86-f509-4235-90b4-c3bbc9f02bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pré-traitement réussi.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 195\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 22\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Lancement du mapping \n",
    "datasets = datasets.map(\n",
    "    prepare_dataset, \n",
    "    remove_columns=datasets.column_names[\"train\"], \n",
    "    num_proc=4,\n",
    "    fn_kwargs={\"processor\": processor}\n",
    ")\n",
    "\n",
    "print(\"Pré-traitement réussi.\")\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faae55eb-b423-431d-afb4-c3fac27b375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49f4d4ec-c4e0-4a87-828b-e13004b487ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c14999d-7b28-4ddf-8a04-1ba5ae97d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8d51aa-9095-4fb6-b819-bc54061bfbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer_result = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer_result = 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer_result, \"cer\": cer_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa615f2a-6898-45b6-a285-67d9a83260dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-docvoice-lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-3,\n",
    "    max_steps=200,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    save_steps=25,\n",
    "    eval_steps=25,\n",
    "    logging_steps=1,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    save_total_limit=2,\n",
    "    greater_is_better=False,\n",
    "    weight_decay=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fdefb59-d55b-4932-af2d-d38c8054e74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beyrem\\AppData\\Local\\Temp\\ipykernel_24772\\525138478.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e54e5263-5f2f-458e-bae2-2323ebc2c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement du fine-tuning LoRA sur le lexique médical...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\Beyrem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 1:05:23, Epoch 15/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.418959</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>6.719368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>0.416920</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.905138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.400924</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>7.509881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.424028</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>7.509881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.418324</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>7.509881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.432583</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>7.905138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.435170</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>7.905138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.436900</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>7.905138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "C:\\Users\\Beyrem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': []}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Beyrem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Beyrem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Beyrem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Beyrem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Beyrem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Beyrem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Beyrem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning terminé. Le modèle est sauvegardé.\n"
     ]
    }
   ],
   "source": [
    "print(\"Lancement du fine-tuning LoRA sur le lexique médical...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Fine-tuning terminé. Le modèle est sauvegardé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1eb2ba3-0416-4777-9917-380d80cbe2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_adapter_output_dir = \"./whisper-docvoice-lora/final_adapter\" \n",
    "peft_model.save_pretrained(final_adapter_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d6b97ad-a8c2-4382-9e9d-fd4fe841997a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51864, 1024, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51864, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from evaluate import load\n",
    "\n",
    "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype,\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# 3. Attach LoRA Adapters (Loading the Checkpoint)\n",
    "# ----------------------------------------------------------------------\n",
    "# This loads the LoRA weights onto the base model\n",
    "model = PeftModel.from_pretrained(base_model, final_adapter_output_dir)\n",
    "model = model.merge_and_unload()\n",
    "# Set the combined model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af29f0f6-18bd-4ba2-b0df-0008646356d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets = load_dataset(\"json\", data_files={\"train\": \"../dataset/processed/json_format/train.json\", \"test\": \"../dataset/processed/json_format/test.json\"})\n",
    "train_dataset = datasets[\"train\"]\n",
    "test_dataset = datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e441506f-b899-48f7-a25e-3214ec7e54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from scipy.io.wavfile import read\n",
    "\n",
    "def predictData(dataset):\n",
    "    pred_texts, ref_texts = [], []\n",
    "    for sample in tqdm(dataset, desc=\"Predecting Speech\"):\n",
    "        audio_path = sample[\"audio\"]\n",
    "        sample_rate, waveform = read(audio_path)\n",
    "        input_features = processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\").input_features.to(torch.float16)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n",
    "        transcription = processor.decode(predicted_ids)\n",
    "        pred_texts.append(processor.tokenizer._normalize(transcription))\n",
    "        ref_texts.append(sample[\"text\"].lower().strip())\n",
    "    return pred_texts,ref_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfa1cc76-0d50-4484-9b90-0791150e0fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predecting Speech: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 55/55 [02:39<00:00,  2.90s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_texts, ref_texts = predictData(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19050062-cceb-4642-a8d7-c22eb6bbbc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper WER (test_dataset): 70.18%\n",
      "Whisper CER (test_dataset): 31.11%\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer, cer\n",
    "baseWer = wer(ref_texts, pred_texts) * 100\n",
    "baseCer = cer(ref_texts, pred_texts) * 100\n",
    "print(f\"Whisper WER (test_dataset): {baseWer:.2f}%\")\n",
    "print(f\"Whisper CER (test_dataset): {baseCer:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92c3fb16-aa68-45bb-b78c-1eacfe84baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predecting Speech: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [09:47<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_texts, ref_texts = predictData(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e85e4a3-da27-44f8-b775-64cedf0c4ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper WER (train_dataset): 74.30%\n",
      "Whisper CER (train_dataset): 28.27%\n"
     ]
    }
   ],
   "source": [
    "baseWer = wer(ref_texts, pred_texts) * 100\n",
    "baseCer = cer(ref_texts, pred_texts) * 100\n",
    "print(f\"Whisper WER (train_dataset): {baseWer:.2f}%\")\n",
    "print(f\"Whisper CER (train_dataset): {baseCer:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c779a-d3aa-4dd7-859c-848210a9866a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
